<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" href="static/images/videoxl.ico.png">
  <title>Video-XL-2</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/videoxl.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/videoxl.ico.png" style="width:1.6em;vertical-align: middle" alt="Logo">
              <span class="Video-XL-2" style="vertical-align: middle">Video-XL-2</span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle" ,="" style="margin-bottom: 20px;">
              A Better, Faster, and High-Frame-Count Model for Long Video Understanding.
            </h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Minghao Qin</a><sup>*</sup>
                <sup>1</sup>,
              </span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Xiangrui Liu</a><sup>*</sup>
                  <sup>1</sup>,
                </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhengyang Liang</a><sup>*</sup>
                    <sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yan Shu</a><sup>1,2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Junjie Zhou</a><sup>1,3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Huaying Yuan</a><sup>1,4</sup>,
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Shitao Xiao</a><sup>1</sup>,
                  </span>
                    <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Bo Zhao</a><sup>1,5</sup>,
                  </span>
                  </span>
                    <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zheng Liu</a><sup>☨</sup><sup>1</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Beijing Academy of Artificial Intelligence</span>
                    <span class="author-block"><sup>2</sup>University of Trento</span>
                    <span class="author-block"><sup>3</sup>Beijing University of Posts and Telecommunications</span>
                    <span class="author-block"><sup>4</sup>Renmin University of China</span>
                    <span class="author-block"><sup>5</sup>Shanghai Jiao Tong University</span>

                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>☨</sup>Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
            
                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/VectorSpaceLab/Video-XL" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                    </span>

                    <!-- Model Huggingface Link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/BAAI/Video-XL-2" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <!-- <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span> -->
                      <span class="icon">
                        🤗
                      </span>
                      <span>Model</span>
                      </a>
                    </span>
                  
                   <!-- Arxiv PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Tech Report (Comming Soon)</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          The field of long video understanding is rapidly evolving. While numerous existing models achieve strong performance on benchmarks, their substantial memory overhead and high response latency become a critical bottleneck, especially as video input lengths grow.
          To overcome these limitations and maintain superior performance, we're releasing Video-XL-2. It makes better and faster long video understanding possible, with key features including:
          </p>
          <ul>
            <li><strong>State-of-the-art performance</strong> among existing open-source models with comparable parameters (as of 2025.6.1).</li>
            <li><strong>High speed and ultra-low memory usage</strong> for processing videos of any length.</li>
          </ul>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- Main statement -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance and Efficiency</h2>
        <div class="content has-text-justified">
          <p>
            Video-XL-2 consistently achieves <strong> state-of-the-art performance </strong> across mainstream Long Video Understanding and Temporal Grounding benchmarks when compared against open-source models with similar parameter counts.
          </p>

          <h3>SOAT Performance</h3>
          <p>
            Video-XL-2 demonstrates <strong> remarkable capabilities </strong> across multiple long video benchmarks, scoring <strong> 74.9 on MLVU, 66.4 on VideoMME, and 48.6 on LVBench</strong>. Notably, Video-XL-2 achieved the <strong>lowest average FLOPs</strong> despite not having the shortest input length during testing, demonstrating its remarkable efficiency. Video-XL-2 also showcases impressive performance in Temporal Grounding task.
          </p>
          <!-- <img src="./static/images/performance.png" alt="motivation" class="inserted-image-noshadow"> -->
          <figure>
            <img src="./static/images/performancev2.png" alt="Video-XL-2-8B 在长视频理解和时间定位基准测试中的性能对比图，展示其超越其他模型的表现。" class="inserted-image-noshadow">
            <figcaption>Table 1: The performance of Video-XL-2 on long video undersatanding benchmarks and temporal grounding benchmarks.</figcaption>
          </figure>


          <h3>Leading Efficiency</h3>
          <p>
              In addition to strong performance, <strong> high efficiency</strong> for processing long videos is another impressive and obvious advantage. We propose two key innovations: <strong> chunk-based pre-filling</strong> and <strong>selective KVS decoding</strong> to largely accelerate it while maintaining performance. As the following figures show, Video-XL-2 delivers exceptional pre-filling speed and memory efficiency. <strong>For a fair comparison, both presented results are obtained under native (eager) attention</strong> .
          </p>

          <div style="display: flex; justify-content: space-around; align-items: flex-start; flex-wrap: wrap;">
              <figure style="flex: 1; min-width: 45%; margin: 10px;">
                  <img src="./static/images/speedv2.png" alt="pre-filling speed" class="inserted-image-noshadow">
                  <figcaption><b>Figure 1:</b> Prefilling Speed Comparison.</figcaption>
              </figure>

              <figure style="flex: 1; min-width: 45%; margin: 10px;">
                  <img src="./static/images/memoryv2.png" alt="memory efficiency" class="inserted-image-noshadow">
                  <figcaption><b>Figure 2:</b> Video Frame Processing Comparison Across Models and GPUs.</figcaption>
              </figure>
          </div>

          </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Architecture and Training Strategy</h2>
        <div class="content has-text-justified">
          <p>
             Designed for long video understanding, Video-XL-2 leverages a powerful model architecture and a multi-stage training approach.
          </p>

          <h3>Architecture Overview</h3>
          <p>
            Video-XL-2 builds upon the robust model architecture of Video-XL Pro, which has demonstrated impressive performance across numerous experiments. 
            Specifically, Video-XL-2 utilizes SigLIP-SO400M as its foundational visual encoder. Following this, a Dynamic Token Synthesize (DTS) Module losslessly compresses visual token sequences by 4x. 
            For the large language model backbone, we've adopted Qwen2.5-Instruct. 
            Further architectural details you can find in our <a href="https://arxiv.org/pdf/2503.18478">https://arxiv.org/pdf/2503.18478</a>.
          </p>
              <figure style="flex: 1; min-width: 45%; margin: 10px;">
                  <img src="./static/images/modelv2.png" alt="memory efficiency" class="inserted-image-noshadow">
                  <figcaption><b>Figure 3:</b> Model Architecture.</figcaption>
             </figure>

          <h3>Training Strategy</h3>
          <p>
            Our training process is divided into four distinct stages, each designed to incrementally build and refine the model's capabilities:
          </p>
          <figure>
              <img src="./static/images/training-strategy.png" alt="training" class="inserted-image-noshadow">
              <figcaption><b>Figure 4</b>: The training strategy of Video-XL-2.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section 4 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Acceleration Approach </h2>
        <div class="content has-text-justified">
          <p>
            Video-XL-2 make a comprehensive acceleration strategy including prefilling and decoding for long video understanding. The strategy is consist
            of two key innovation techniques: Chunk-based pre-filling and Bi-level KVs Decoding. These approaches make our models achieve low memory demanding and high speed on both pre-filling and decoding.
          </p>

        <h3>Prefilling: Chunk-based pre-filling</h3>
        <p>
            For prefilling acceleration, previous works have oberseve the attention pattern in video LLM is super sparse. Based on these observation, we <strong> encode
            video chunks one by one</strong>, and made the timpestampe to be the information carrier, each timestamps betwee chunks summary the history information and pass it to the later chunks. This design make Video-XL-2 largely reduce the
            attention flops. The above design can be summaried as the Figure 5.
        </p>
          <figure style="flex: 1; min-width: 45%; margin: 10px;">
              <img src="./static/images/prefill.png" alt="memory efficiency" class="inserted-image-noshadow">
              <figcaption><b>Figure 5:</b>Chunk-based pre-filling.</figcaption>
          </figure>

        <h3>Dedoding: Bi-level KVs Decoding</h3>
        <p>
           After prefilling, all KVs prepared in cache for subsequent decoding. The most cases in long video understanding is only fouces on single or multi time spans in
           long video. So, based on this, we select <strong> dense KVs </strong> of important video chunks for providing detailed local information and downsample the other KVs to <strong>sparse KVs</strong> for reducing KV cache demanding and provide coarse
           -granularity abstract global information. Through this approach, Video-XL-2 counld get better performance in detail undersatanding and high efficiency in decoding.
        </p>
          <figure style="flex: 1; min-width: 45%; margin: 10px;">
              <img src="./static/images/decoding.png" alt="memory efficiency" class="inserted-image-noshadow">
              <figcaption><b>Figure 6:</b> Bi-level KVs Decoding.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- Section 4-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{shu2024video,
  title={Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding},
  author={Shu, Yan and Zhang, Peitian and Liu, Zheng and Qin, Minghao and Zhou, Junjie and Huang, Tiejun and Zhao, Bo},
  journal={arXiv preprint arXiv:2409.14485},
  year={2024}
}

@article{liu2025video,
  title={Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding},
  author={Liu, Xiangrui and Shu, Yan and Liu, Zheng and Li, Ao and Tian, Yang and Zhao, Bo},
  journal={arXiv preprint arXiv:2503.18478},
  year={2025}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
