<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Video-XL2: the fastest long video understander.</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Video-XL2: The Better And Faster Long Video Understander.</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Minji</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Hanni</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Daniel</a>
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Haerin</a>
                  </span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hyein</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Newjeans</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper(Comming Soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Demo Show</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Main statement -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video-XL2</h2>
        <div class="content has-text-justified">
          <p>
          The field of long video understanding is rapidly evolving. While numerous existing models achieve strong performance on benchmarks, their substantial memory overhead and high response latency become a critical bottleneck, especially as video input lengths grow. 
          To definitively overcome these limitations and maintain superior performance, we're releasing Video-XL2. It makes faster and better long video understanding possible, with key features including:
          State-of-the-art performance among existing open-source models with comparable parameters (as of 2025.6.1).
          High speed and ultra-low memory usage for processing videos of any length.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video-XL2</h2>
        <div class="content has-text-justified">
          <p>
          The field of long video understanding is rapidly evolving. While numerous existing models achieve strong performance on benchmarks, their substantial memory overhead and high response latency become a critical bottleneck, especially as video input lengths grow.
          To definitively overcome these limitations and maintain superior performance, we're releasing Video-XL2. It makes faster and better long video understanding possible, with key features including:
          </p>
          <ul>
            <li><strong>State-of-the-art performance</strong> among existing open-source models with comparable parameters (as of 2025.6.1).</li>
            <li><strong>High speed and ultra-low memory usage</strong> for processing videos of any length.</li>
          </ul>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- Main statement -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">State-of-the-Art Performance in Long Video Understanding and Temporal Grounding</h2>
        <div class="content has-text-justified">
          <p>
            Video-XL2-8B consistently achieves **state-of-the-art performance** across mainstream Long Video Understanding and Temporal Grounding benchmarks when compared against open-source models with similar parameter counts.
          </p>

          <h3>Long Video Understanding</h3>
          <p>
            Video-XL2-8B demonstrates **remarkable capabilities** across multiple long video benchmarks, scoring **74.9 on MLVU, 66.4 on VideoMME, and 60.7 on LongVideoBench**.
          </p>
          <!-- <img src="images/carousel1.jpg" alt="motivation" class="inserted-image-noshadow"> -->

          <h3>Temporal Grounding</h3>
          <p>
            Video-XL2-8B showcases **impressive performance** in Temporal Grounding, establishing itself as the *the best general VLM** for this task. Video-XL2-8B achieved **XX on TimePass, XX on Charades-STA, and XX on VISTAL**. Notably, it **surpasses 72B models** on some benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Section 2 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Superior Efficiency</h2>
        <div class="content has-text-justified">
          <p>
            In addition to strong performance, the other impressive and obvious advantage is high efficiency for processing long video. We propose two key innovation (block-granularity pre-filiing, selecting decoding) 
            to largely accelerate it and maintain its performance. As a result, Video-XL2 make remarkable reduction on prefilling FLOPS and KV Cache demanding in decoding and reduce the TTFT.
          </p>
          <!-- <img src="images/carousel3.jpg" alt="motivation" class="inserted-image-noshadow"> -->
          <p>
            This advantage is more obvious in extremely-long scenario or long resource scenario. As the following Fig 3 shows, compared to Video-ChatFlash, Video-XL2 still maintain high prefilling speed in thougnds frames input.
            At the same time, its maintain low memory resource demanding. This point make our models better for these special but more important and realistic scenario.
          </p>
          <!-- <img src="images/carousel3.jpg" alt="motivation" class="inserted-image-noshadow"> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Section 2-->



<!-- Section 3 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Architecture And Training Strategy</h2>
        <div class="content has-text-justified">
          <p>
            1. 模型架构：要包含时间戳和 time embedding
            2. 简单说一下训练阶段的设计
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Section 3-->



<!-- Section 4 -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Acceleration Approach </h2>
        <div class="content has-text-justified">
          <p>
            Video-XL2 make a comprehensive acceleration strategy including prefilling and decoding for long video understanding. The strategy is consist
            of two key innovation techniques: Block-granularity Sparsity Attention and Bi-level KV Decoding. These approaches make our models achieve low memory demanding and high speed on both pre-filling and decoding.
          </p>
        

        <h3>Prefilling: Block-granularity Sparsity Attention</h3>
        <p>
            For prefilling acceleration, previous works have oberseve the attention pattern in video LLM is super sparse. Based on these observation, we streamingly encode
            video chunks one by one, and provided with long term memory and short term memory for preserving complete video contextual structure. For short term information, we 
            apply Block-granularity sliding window mechanism to make each chunk can aggregate the information of n chunks before it. For Long Term, the timpestampe was made to be
            the information carrier, each timestamps betwee chunks summary the history information and pass it to the later chunks. This design make Video-XL2 largely reduce the
            attention flops. The above design can be summaried as the Figure 4.
        </p>

        <h3>Dedoding: Bi-level KV Decoding</h3>
        <p>
           After prefilling, all KVs prepared in cache for subsequent decoding. The most cases in long video understanding is only fouces on single or multi time spans in
           long video. So, based on this, we select KVs of  important video chunks for providing detailed local information and merge the other KVs to reduce KV cache demanding and provide coarse
           -granularity abstract global information. Through this approach, Video-XL2 counld get better performance in detail undersatanding and high efficiency in decoding.
        </p>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- Section 4-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
